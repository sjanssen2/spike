rule varscan_trio:
    # Preprocessing to do a realignment of the reads with GATK
    input:
        exometrack=lambda wildcards: "%s%s%s" % (config['dirs']['prefix'], config['dirs']['references'], get_reference_exometrack(get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'patient', SAMPLESHEETS), SAMPLESHEETS, config)),
        references=lambda wildcards: ["%s%s%s%s" % (wildcards.prefix, config['dirs']['references'], get_reference_genome(get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'patient', SAMPLESHEETS), SAMPLESHEETS, config)['file'], ending) for ending in config['endings_bwa']],
        patient=lambda wildcards: "%s%s%s/%s.reCal.reAl.nodup.srt.bam" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['gatk_PrintReads'], get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'patient', SAMPLESHEETS)),
        father=lambda wildcards: "%s%s%s/%s.reCal.reAl.nodup.srt.bam" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['gatk_PrintReads'], get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'father', SAMPLESHEETS)),
        mother=lambda wildcards: "%s%s%s/%s.reCal.reAl.nodup.srt.bam" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['gatk_PrintReads'], get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'mother', SAMPLESHEETS)),
    output:
        indel="{prefix}%s%s/{trio}.indel.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_trio']),
        snp="{prefix}%s%s/{trio}.snp.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_trio']),
    log:
        samtools="{prefix}%s%s/{trio}.samtools.log" % (config['dirs']['logs'], config['stepnames']['varscan_trio']),
        varscan="{prefix}%s%s/{trio}.varscan.log" % (config['dirs']['logs'], config['stepnames']['varscan_trio'])
    benchmark:
        "{prefix}%s%s/{trio}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['varscan_trio'])
    conda:
        "envs/spike_varscan.yaml"
    threads:
        1  # since samtools doesn't support multithreading
    params:
        workdir="{prefix}%s%s/{trio}" % (config['dirs']['intermediate'], config['stepnames']['varscan_trio'])
    shell:
        #set the locale to C which avoids problems with . and , used as decimal separator
        "export LANG=C && "
        "mkdir -v -p {params.workdir} > {log.samtools}"
        " && samtools"
        " mpileup"
        " -B "
        " -q 1"
        " -f {input.references[0]}"
        " -l {input.exometrack}"
        " {input.father}"
        " {input.mother}"
        " {input.patient}"
        " 2>> {log.samtools}"
        " | java"
        " -Xmx3g"
        " -XX:ParallelGCThreads={threads}"
        " -jar $CONDA_PREFIX/share/varscan-2.4.3-1/VarScan.jar"
        " trio"
        " - {params.workdir}"
        " --min-coverage 10"
        " --min-var-freq 0.20"
        " --p-value 0.05"
        " -adj-var-freq 0.05"
        " -adj-p-value 0.15"
        " 2> {log.varscan}"
        " && rmdir -v {params.workdir} >> {log.varscan}"

        # ##################################
        # # STEP PRODUCING MPILEUP FOR TRIOS
        # ##################################
        # echo launching mpileup and passing to varscanfor $OUTPUTBASENAME
        # # $EXOME_TRACK is set in common.env
        # /data/biotools/src/samtools-1.3/samtools mpileup -B -q 1 -f $REF -l $EXOME_TRACK $FATHER_BAM $MOTHER_BAM $CHILD_BAM |java -Xmx3g -XX:ParallelGCThreads=1 -Djava.io.tmpdir=$TEMPPATH \
        # -jar $VARSCAN2JAR trio - $OUTPUTBASENAME --min-coverage 10 --min-var-freq 0.20 --p-value 0.05 -adj-var-freq 0.05 -adj-p-value 0.15


rule varscan_filter_SNP:
    # STEP FILTER
    input:
        snp="{prefix}%s%s/{trio}.snp.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_trio']),
        indel="{prefix}%s%s/{trio}.indel.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_trio'])
    output:
        "{prefix}%s%s/{trio}.snp.filtered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_filter']),
    log:
        "{prefix}%s%s_SNP/{trio}.log" % (config['dirs']['logs'], config['stepnames']['varscan_filter']),
    benchmark:
        "{prefix}%s%s_SNP/{trio}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['varscan_filter']),
    conda:
        "envs/spike_varscan.yaml"
    threads:
        1
    shell:
        #set the locale to C which avoids problems with . and , used as decimal separator
        "export LANG=C && "
        "java"
        " -Xmx3g"
        " -XX:ParallelGCThreads={threads}"
        " -jar $CONDA_PREFIX/share/varscan-2.4.3-1/VarScan.jar"
        " filter"
        " {input.snp}"
        " --min-coverage 20"
        " --min-reads2 2"
        " --min-strands2 2"
        " --min-avg-qual 20"
        " --min-var-freq 0.2"
        " --p-value 1e-01"
        " --indel-file {input.indel}"
        " --output-file {output}"
        " 2> {log}"

        # ################
        # # STEP FILTER
        # ################
        # #filtering: filter for the snps
        # echo filtering snps for $OUTPUTBASENAME
        # java -Xmx3g -XX:ParallelGCThreads=1 -Djava.io.tmpdir=$TEMPPATH -jar $VARSCAN2JAR filter $OUTPUTBASENAME.snp.vcf --min-coverage 20 --min-reads2 2 --min-strands2 2 --min-avg-qual 20 --min-var-freq 0.2 --p-value 1e-01 --indel-file $OUTPUTBASENAME.indel.vcf --output-file $OUTPUTBASENAME.snp.filtered.vcf


rule varscan_filter_INDEL:
    # STEP FILTER
    input:
        "{prefix}%s%s/{trio}.indel.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_trio'])
    output:
        "{prefix}%s%s/{trio}.indel.filtered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_filter']),
    log:
        "{prefix}%s%s_INDEL/{trio}.log" % (config['dirs']['logs'], config['stepnames']['varscan_filter']),
    benchmark:
        "{prefix}%s%s_INDEL/{trio}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['varscan_filter']),
    conda:
        "envs/spike_varscan.yaml"
    threads:
        1
    shell:
        #set the locale to C which avoids problems with . and , used as decimal separator
        "export LANG=C && "
        "java"
        " -Xmx3g"
        " -XX:ParallelGCThreads={threads}"
        " -jar $CONDA_PREFIX/share/varscan-2.4.3-1/VarScan.jar"
        " filter"
        " {input}"
        " --min-coverage 20"
        " --min-reads2 2"
        " --min-strands2 2"
        " --min-avg-qual 20"
        " --min-var-freq 0.2"
        " --output-file {output}"
        " 2> {log}"

        # ################
        # # STEP FILTER
        # ################
        # #for the indels
        # echo filtering indels for $OUTPUTBASENAME
        # java -Xmx3g -XX:ParallelGCThreads=1 -Djava.io.tmpdir=$TEMPPATH -jar $VARSCAN2JAR filter $OUTPUTBASENAME.indel.vcf --min-coverage 20 --min-reads2 2 --min-strands2 2 --min-avg-qual 20 --min-var-freq 0.2 --p-value 1e-01 --output-file $OUTPUTBASENAME.indel.filtered.vcf


rule bam_readcount_SNP:
    input:
        references=lambda wildcards: ["%s%s%s%s" % (wildcards.prefix, config['dirs']['references'], get_reference_genome(get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'patient', SAMPLESHEETS), SAMPLESHEETS, config)['file'], ending) for ending in config['endings_bwa']],
        vcf="{prefix}%s%s/{trio}.snp.filtered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_filter']),
        patient=lambda wildcards: "%s%s%s/%s.reCal.reAl.nodup.srt.bam" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['gatk_PrintReads'], get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'patient', SAMPLESHEETS)),
    output:
        var="{prefix}%s%s/{trio}.snp.filtered.var" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount']),
        readcount="{prefix}%s%s/{trio}.snp.readcount" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount'])
    log:
        "{prefix}%s%s_SNP/{trio}.log" % (config['dirs']['logs'], config['stepnames']['bam_readcount']),
    benchmark:
        "{prefix}%s%s_SNP/{trio}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['bam_readcount']),
    conda:
        "envs/spike_bam-readcount.yaml"
    threads:
        1
    shell:
        'cat {input.vcf}'
        ' | grep -v "#"'
        ' | cut -f 1,2'
        " | awk '{{ print $1, $2, $2 }}'"
        ' > {output.var}'
        ' 2> {log}'
        ' && bam-readcount'
        ' -q 1'
        ' -b 20'
        ' -w 1'
        ' -l {output.var}'
        ' -f {input.references[0]}'
        ' {input.patient}'
        ' > {output.readcount}'
        ' 2>> {log}'

        # echo prepare variant list for fpfilter for $OUTPUTBASENAME
        # cat $OUTPUTBASENAME.snp.filtered.vcf | grep -v "#"| cut -f1,2 | awk '{ print $1, $2, $2 }' > $OUTPUTBASENAME.snp.filtered.var
        # #launch bam readcount for the indels
        # echo
        # echo running /data/biotools/bin/bam-readcount -q 1 -b 20 -w 1 -l $OUTPUTBASENAME.snp.filtered.var -f $REF $CHILD_BAM > $CHILD_BAM.snp.readcount
        # /data/biotools/bin/bam-readcount -q 1 -b 20 -w 1 -l $OUTPUTBASENAME.snp.filtered.var -f $REF $CHILD_BAM > $CHILD_BAM.snp.readcount
        # #seems to work


rule bam_readcount_INDEL:
    input:
        references=lambda wildcards: ["%s%s%s%s" % (wildcards.prefix, config['dirs']['references'], get_reference_genome(get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'patient', SAMPLESHEETS), SAMPLESHEETS, config)['file'], ending) for ending in config['endings_bwa']],
        vcf="{prefix}%s%s/{trio}.indel.filtered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_filter']),
        patient=lambda wildcards: "%s%s%s/%s.reCal.reAl.nodup.srt.bam" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['gatk_PrintReads'], get_role(wildcards.trio.split('/')[0], wildcards.trio.split('/')[-1], 'patient', SAMPLESHEETS)),
    output:
        var="{prefix}%s%s/{trio}.indel.filtered.var" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount']),
        readcount="{prefix}%s%s/{trio}.indel.readcount" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount'])
    log:
        "{prefix}%s%s_INDEL/{trio}.log" % (config['dirs']['logs'], config['stepnames']['bam_readcount']),
    benchmark:
        "{prefix}%s%s_INDEL/{trio}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['bam_readcount']),
    conda:
        "envs/spike_bam-readcount.yaml"
    threads:
        1
    shell:
        "awk 'BEGIN {{OFS=\"\t\"}} {{if (!/^#/) {{ isDel=(length($4) > length($5)) ? 1 : 0; print $1,($2+isDel),($2+isDel); }}}}'"
        " {input.vcf}"
        " > {output.var}"
        ' 2> {log}'
        ' && bam-readcount'
        ' -q 1'
        ' -b 20'
        ' -w 1'
        ' -l {output.var}'
        ' -f {input.references[0]}'
        ' {input.patient}'
        ' > {output.readcount}'
        ' 2>> {log}'

        # #now do the same for the indels
        # #taken from the varscan website
        # echo preparing readcounts for indels from $OUTPUTBASENAME
        # awk 'BEGIN {OFS="\t"} {if (!/^#/) { isDel=(length($4) > length($5)) ? 1 : 0; print $1,($2+isDel),($2+isDel); }}' $OUTPUTBASENAME.indel.filtered.vcf > $OUTPUTBASENAME.indel.filtered.var
        # echo /data/biotools/bin/bam-readcount -q 1 -b 20 -w 1 -l $OUTPUTBASENAME.indel.filtered.var -f $REF $CHILD_BAM > $CHILD_BAM.indel.readcount
        # /data/biotools/bin/bam-readcount -q 1 -b 20 -w 1 -l $OUTPUTBASENAME.indel.filtered.var -f $REF $CHILD_BAM > $CHILD_BAM.indel.readcount


rule varscan_fpfilter:
    # fpfilter
    input:
        var="{prefix}%s%s/{trio}.{snvtype}.filtered.var" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount']),
        readcount="{prefix}%s%s/{trio}.{snvtype}.readcount" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount']),
        vcf="{prefix}%s%s/{trio}.{snvtype}.filtered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_filter']),
    output:
        vcf="{prefix}%s%s/{trio}.{snvtype,snp|indel}.fpfiltered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_fpfilter']),
        ogus="{prefix}%s%s/{trio}.{snvtype,snp|indel}.ogus.txt" % (config['dirs']['intermediate'], config['stepnames']['varscan_fpfilter']),
    log:
        "{prefix}%s%s/{trio}.{snvtype}.log" % (config['dirs']['logs'], config['stepnames']['varscan_fpfilter']),
    benchmark:
        "{prefix}%s%s/{trio}.{snvtype}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['varscan_fpfilter'])
    conda:
        "envs/spike_varscan.yaml"
    threads:
        1
    shell:
        #set the locale to C which avoids problems with . and , used as decimal separator
        "export LANG=C && "
        "java"
        " -Xmx3g"
        " -XX:ParallelGCThreads={threads}"
        " -jar $CONDA_PREFIX/share/varscan-2.4.3-1/VarScan.jar"
        " fpfilter"
        " {input.vcf}"
        " {input.readcount}"
        " --min-var-count 3"
        " --min-strandedness 0"
        " --min-var-basequal 30"
        " --min-ref-readpos 0.20"
        " --min-ref-dist3 0.20"
        " --min-var-readpos 0.15"
        " --min-var-dist3 0.15"
        " --max-rl-diff 0.05"
        " --max-mapqual-diff 10"
        " --min-ref-mapqual 20"
        " --min-var-mapqual 30"
        " --max-var-mmqs 100"
        " --max-ref-mmqs 50"
        ' $(if [ "{wildcards.snvtype}" == "indel" ]; then echo " --min-var-count-lc 1"; fi)'
        " --output-file {output.vcf}"
        " --filtered-file {output.ogus}"
        ' 2>> {log}'

        # #The fpfilter parameters resemble what the varscan team did in the dream3-challenge
        # # With the following changes:
        # # --min-var-count-lc 2 removed, since this is not a tumor/normal scenario
        # echo launching fpfilter for $OUTPUTBASENAME
        # echo running fpfilter
        # java -Xmx3g -XX:ParallelGCThreads=1 -Djava.io.tmpdir=$TEMPPATH -jar $VARSCAN2JAR fpfilter $OUTPUTBASENAME.snp.filtered.vcf $CHILD_BAM.snp.readcount \
        # --min-var-count 3 --min-strandedness 0 --min-var-basequal 30 --min-ref-readpos 0.20 --min-ref-dist3 0.20 \
        # --min-var-readpos 0.15 --min-var-dist3 0.15 --max-rl-diff 0.05 --max-mapqual-diff 10 --min-ref-mapqual 20 --min-var-mapqual 30 --max-var-mmqs 100 \
        # --max-ref-mmqs 50 --output-file $OUTPUTBASENAME.snp.fpfiltered.vcf --filtered-file ogus.txt
        # # works for the snps

        # echo
        # echo running flfilter for indels for $OUTPUTBASENAME
        # java -Xmx3g -XX:ParallelGCThreads=1 -Djava.io.tmpdir=$TEMPPATH -jar $VARSCAN2JAR fpfilter $OUTPUTBASENAME.indel.filtered.vcf $CHILD_BAM.indel.readcount \
        # --min-var-count 3 --min-var-count-lc 1 --min-strandedness 0 --min-var-basequal 30 --min-ref-readpos 0.20 --min-ref-dist3 0.20 \
        # --min-var-readpos 0.15 --min-var-dist3 0.15 --max-rl-diff 0.05 --max-mapqual-diff 10 --min-ref-mapqual 20 --min-var-mapqual 30 --max-var-mmqs 100 \
        # --max-ref-mmqs 50 --output-file $OUTPUTBASENAME.indel.fpfiltered.vcf --filtered-file ogus.txt


rule correct_genotypes:
    #correct genotypes
    input:
        "{prefix}%s%s/{trio}.{snvtype}.{suffix}.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_fpfilter']),
    output:
        vcf="{prefix}%s%s/{trio}.{snvtype,snp|indel}.{suffix}.corrected.vcf" % (config['dirs']['intermediate'], config['stepnames']['correct_genotypes']),
    log:
        "{prefix}%s%s/{trio}.{snvtype}.{suffix}.log" % (config['dirs']['logs'], config['stepnames']['correct_genotypes']),
    benchmark:
        "{prefix}%s%s/{trio}.{snvtype}.{suffix}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['correct_genotypes'])
    threads:
        1
    run:
        with open(str(input), 'r') as f_in:
            with open(str(output), 'w') as f_out:
                for line in f_in:
                    if not line or line[0] is "#":
                        f_out.write(line)
                    else:
                        fields = line.split("\t")
                        fields[3] = fields[3].replace("/", ",").replace("\\", ",")   # remove any slashes from REF field
                        fields[4] = fields[4].replace("/", ",").replace("\\", ",")   # remove any slashes from ALT field
                        f_out.write("\t".join(fields))

        # #correct genotypes
        # echo correcting genotypes for $OUTPUTBASENAME
        # python /data/biotools/scripts/correct_varscan_genotypes.py $OUTPUTBASENAME.snp.fpfiltered.vcf $OUTPUTBASENAME.snp.fpfiltered.corrected.vcf
        # python /data/biotools/scripts/correct_varscan_genotypes.py $OUTPUTBASENAME.indel.fpfiltered.vcf $OUTPUTBASENAME.indel.fpfiltered.corrected.vcf


rule correct_genotypes_somatic:
    #correct genotypes
    input:
        lambda wildcards: "%s%s%s/%s.%s%s.vcf" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['varscan_somatic'] if wildcards.mutationclass=="" else config['stepnames']['varscan_processSomatic'], wildcards.entity, wildcards.snvtype, wildcards.mutationclass),
    output:
        vcf="{prefix}%s%s/{entity}.{snvtype,snp|indel}{mutationclass,\.Somatic\.hc|\.Germline\.hc|}.corrected.vcf" % (config['dirs']['intermediate'], config['stepnames']['correct_genotypes']),
    log:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.log" % (config['dirs']['logs'], config['stepnames']['correct_genotypes']),
    benchmark:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['correct_genotypes'])
    threads:
        1
    run:
        with open(str(input), 'r') as f_in:
            with open(str(output), 'w') as f_out:
                for line in f_in:
                    if not line or line[0] is "#":
                        f_out.write(line)
                    else:
                        fields = line.split("\t")
                        fields[3] = fields[3].replace("/", ",").replace("\\", ",")   # remove any slashes from REF field
                        fields[4] = fields[4].replace("/", ",").replace("\\", ",")   # remove any slashes from ALT field
                        f_out.write("\t".join(fields))

        # SMJ: changing position of .corrected. :
        # python /data/biotools/scripts/correct_varscan_genotypes.py $OUTPUTBASENAME.snp.Somatic.hc.vcf $OUTPUTBASENAME.snp.Somatic.hc.corrected.vcf
        # python /data/biotools/scripts/correct_varscan_genotypes.py $OUTPUTBASENAME.snp.Germline.hc.vcf $OUTPUTBASENAME.snp.Germline.hc.corrected.vcf


rule merge_vcfs:
    #merge the vcfs
    input:
        snp="{prefix}%s%s/{trio}.snp.fpfiltered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_fpfilter']),
        indel="{prefix}%s%s/{trio}.indel.fpfiltered.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_fpfilter']),
    output:
        "{prefix}%s%s/{trio}.fpcorr.snp_indel.vcf" % (config['dirs']['intermediate'], config['stepnames']['merge_vcfs']),
    log:
        "{prefix}%s%s/{trio}.log" % (config['dirs']['logs'], config['stepnames']['merge_vcfs']),
    benchmark:
        "{prefix}%s%s/{trio}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['merge_vcfs'])
    conda:
        "envs/spike_vcftools.yaml"
    threads:
        1
    shell:
        "vcf-concat"
        " {input.snp}"
        " {input.indel}"
        ' > {output}'
        " 2> {log}"
        " && chmod -v g+w {output} 2>> {log}"

        # #merge the vcfs
        # echo merging vcfs for $OUTPUTBASENAME
        # vcf-concat $OUTPUTBASENAME.snp.fpfiltered.fixed.vcf $OUTPUTBASENAME.indel.fpfiltered.fixed.vcf > "$OUTPUTBASENAME.fpcorr.snp_indel.vcf" # $OUTPUTBASENAME.loh.corrected.varscan.somatic.vcf


rule writing_headers:
    #merge the vcfs
    input:
        rules.merge_vcfs.output
    output:
        "{prefix}%s%s/{trio}.var2denovo.vcf" % (config['dirs']['intermediate'], config['stepnames']['writing_headers']),
    log:
        "{prefix}%s%s/{trio}.log" % (config['dirs']['logs'], config['stepnames']['writing_headers']),
    benchmark:
        "{prefix}%s%s/{trio}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['writing_headers'])
    conda:
        "envs/spike_vcftools.yaml"
    threads:
        1
    shell:
        "cat "
        " {input}"
        ' | grep'
        ' "#" > {output}'
        ' && cat {input}'
        ' | grep'
        ' "STATUS=3" '
        ' | sort '
        ' -k1,1 -n'
        ' -k2,2 -n'
        ' >> {output}'
        " 2> {log}"

        # #writing the final header
        # echo limiting to denovos and sorting for $OUTPUTBASENAME
        # cat $OUTPUTBASENAME.fpcorr.snp_indel.vcf | grep "#" >$OUTPUTBASENAME.var2denovo.vcf
        # #selecting for denovos via their status to avoid grepping a headerline too much
        # cat $OUTPUTBASENAME.fpcorr.snp_indel.vcf | grep "STATUS=3" | sort -k1,1 -n -k2,2 -n >>$OUTPUTBASENAME.var2denovo.vcf


#### rules for somatic calling

rule samtools_mpileup_somatic:
    # Preprocessing to do a realignment of the reads with GATK
    input:
        exometrack=lambda wildcards: "%s%s%s" % (config['dirs']['prefix'], config['dirs']['references'], get_reference_exometrack(get_role(wildcards.entity.split('/')[0], wildcards.entity.split('/')[-1], wildcards.role, SAMPLESHEETS), SAMPLESHEETS, config)),
        references=lambda wildcards: ["%s%s%s%s" % (wildcards.prefix, config['dirs']['references'], get_reference_genome(get_role(wildcards.entity.split('/')[0], wildcards.entity.split('/')[-1], wildcards.role, SAMPLESHEETS), SAMPLESHEETS, config)['file'], ending) for ending in config['endings_bwa']],
        bam=lambda wildcards: "%s%s%s/%s.reCal.reAl.nodup.srt.bam" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['gatk_PrintReads'], get_role(wildcards.entity.split('/')[0], wildcards.entity.split('/')[-1], wildcards.role, SAMPLESHEETS)),
    output:
        "{prefix}%s%s/{entity}.{role,healthy|tumor}.pileup" % (config['dirs']['intermediate'], config['stepnames']['samtools_mpileup'])
    log:
        "{prefix}%s%s/{entity}.{role}.log" % (config['dirs']['logs'], config['stepnames']['samtools_mpileup']),
    benchmark:
        "{prefix}%s%s/{entity}.{role}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['samtools_mpileup'])
    conda:
        "envs/spike_varscan.yaml"
    threads:
        1
    shell:
        "samtools"
        " mpileup"
        " -f {input.references[0]}"
        " -q 1"
        " -l {input.exometrack}"
        " -B {input.bam}"
        " > {output}"
        " 2> {log}"

        # /data/biotools/src/samtools-1.3/samtools mpileup -f $REF -q1 -l $EXOME_TRACK -B $NORMAL_BAM > $NORMAL_PILEUP || usage "error ($?) reading normal bam file with samtools: $NORMAL_BAM"
        # /data/biotools/src/samtools-1.3/samtools mpileup -f $REF -q1 -l $EXOME_TRACK -B $TUMOR_BAM > $TUMOR_PILEUP || usage "error ($?) reading normal bam file with samtools: $TUMOR_BAM"


rule varscan_somatic:
    # Preprocessing to do a realignment of the reads with GATK
    input:
        tumor="{prefix}%s%s/{entity}.tumor.pileup" % (config['dirs']['intermediate'], config['stepnames']['samtools_mpileup']),
        healthy="{prefix}%s%s/{entity}.healthy.pileup" % (config['dirs']['intermediate'], config['stepnames']['samtools_mpileup']),
    output:
        indel="{prefix}%s%s/{entity}.indel.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_somatic']),
        snp="{prefix}%s%s/{entity}.snp.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_somatic']),
    log:
        "{prefix}%s%s/{entity}.log" % (config['dirs']['logs'], config['stepnames']['varscan_somatic'])
    benchmark:
        "{prefix}%s%s/{entity}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['varscan_somatic'])
    conda:
        "envs/spike_varscan_somatic.yaml"
    threads:
        1
    params:
        speciesParams=lambda wildcards: get_reference_varscan_somatic(get_role(wildcards.entity.split('/')[0], wildcards.entity.split('/')[-1], 'tumor', SAMPLESHEETS), SAMPLESHEETS, config),
        outdir=lambda wildcards: "%s%s%s/%s" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['varscan_somatic'], wildcards.entity)
    shell:
        #set the locale to C which avoids problems with . and , used as decimal separator
        "export LANG=C && "
        "java"
        " -Xmx3g"
        " -XX:ParallelGCThreads={threads}"
        " -jar $CONDA_PREFIX/share/varscan-2.4.1-0/VarScan.jar"
        " somatic"
        " {input.healthy}"
        " {input.tumor}"
        " {params.outdir}"
        " {params.speciesParams}"
        " --min-var-freq 0.1"
        " --normal-purity 0.95"
        " --strand-filter 1"
        " --output-vcf 1"
        " 2>> {log}"

        # java -Xmx3g -XX:ParallelGCThreads=1 -Djava.io.tmpdir=$TEMPPATH -jar $VARSCAN2JAR  somatic $NORMAL_PILEUP $TUMOR_PILEUP $OUTPUTBASENAME --min-coverage-normal 8 --min-coverage-tumor 10 --min-reads 4 --min-var-freq 0.1 --normal-purity 0.95 --strand-filter 1 --output-vcf 1 -- $*


rule varscan_processSomatic:
    # Preprocessing to do a realignment of the reads with GATK
    input:
        "{prefix}%s%s/{entity}.{snvtype}.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_somatic']),
    output:
        somatic_hc="{prefix}%s%s/{entity}.{snvtype,snp|indel}.Somatic.hc.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_processSomatic']),
        somatic="{prefix}%s%s/{entity}.{snvtype,snp|indel}.Somatic.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_processSomatic']),
        germline_hc="{prefix}%s%s/{entity}.{snvtype,snp|indel}.Germline.hc.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_processSomatic']),
        germline="{prefix}%s%s/{entity}.{snvtype,snp|indel}.Germline.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_processSomatic']),
        loh_hc="{prefix}%s%s/{entity}.{snvtype,snp|indel}.LOH.hc.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_processSomatic']),
        loh="{prefix}%s%s/{entity}.{snvtype,snp|indel}.LOH.vcf" % (config['dirs']['intermediate'], config['stepnames']['varscan_processSomatic']),
    log:
        "{prefix}%s%s/{entity}.{snvtype}.log" % (config['dirs']['logs'], config['stepnames']['varscan_processSomatic'])
    benchmark:
        "{prefix}%s%s/{entity}.{snvtype}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['varscan_processSomatic'])
    conda:
        "envs/spike_varscan.yaml"
    threads:
        1
    shell:
        # first move input file, to output directory, because output directory cannot be defined for this command and I don't want to mess up the real input directory
        "cp -v -l {input} $(dirname {output.somatic_hc}) 2>> {log} 1>&2"
        " && java"
        " -jar $CONDA_PREFIX/share/varscan-2.4.3-1/VarScan.jar"
        " processSomatic"
        " $(dirname {output.somatic_hc})/$(basename {input})"
        " 2>> {log}"
        # delete moved input file after process finished
        " && rm -v -f $(dirname {output.somatic_hc})/$(basename {input}) 2>> {log} 1>&2"

        # java -jar $VARSCAN2JAR processSomatic $OUTPUTBASENAME.snp.vcf ;
        # java -jar $VARSCAN2JAR processSomatic $OUTPUTBASENAME.indel.vcf
        # processSomatic: from http://varscan.sourceforge.net/somatic-calling.html
        # The above command will produce 4 output files:
        # output.snp.Somatic.hc 	(high-confidence Somatic mutations)
        # output.snp.Somatic.lc 	(low-confidence Somatic mutations)
        # output.snp.Germline 	(sites called Germline)
        # output.snp.LOH 		(sites called loss-of-heterozygosity, or LOH)


rule varscan_fpfilter_somatic:
    # fpfilter
    input:
        lambda wildcards: "%s%s%s/%s.%s%s.vcf" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['varscan_somatic'] if wildcards.mutationclass=="" else config['stepnames']['varscan_processSomatic'], wildcards.entity, wildcards.snvtype, wildcards.mutationclass),
    output:
        "{prefix}%s%s/{entity}.{snvtype,snp|indel}{mutationclass,\.Somatic\.hc|\.Germline\.hc|}.vcf.fpfilter.var" % (config['dirs']['intermediate'], config['stepnames']['varscan_fpfilter']),
    log:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.log" % (config['dirs']['logs'], config['stepnames']['varscan_fpfilter']),
    benchmark:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['varscan_fpfilter'])
    threads:
        1
    shell:
        "perl"
        " -ane"
        ' \'print join("\t",@F[0,1,1])."\n" unless(m/^#/)\''
        " {input}"
        " > {output}"

        # perl -ane 'print join("\t",@F[0,1,1])."\n" unless(m/^#/)' $OUTPUTBASENAME.snp.Somatic.hc.vcf > $OUTPUTBASENAME.snp.Somatic.hc.vcf.fpfilter.var
        # perl -ane 'print join("\t",@F[0,1,1])."\n" unless(m/^#/)' $OUTPUTBASENAME.snp.Germline.hc.vcf > $OUTPUTBASENAME.snp.Germline.hc.vcf.fpfilter.var


rule bam_readcount_somatic_SNP:
    input:
        references=lambda wildcards: ["%s%s%s%s" % (wildcards.prefix, config['dirs']['references'], get_reference_genome(get_role(wildcards.entity.split('/')[0], wildcards.entity.split('/')[-1], 'tumor', SAMPLESHEETS), SAMPLESHEETS, config)['file'], ending) for ending in config['endings_bwa']],
        var="{prefix}%s%s/{entity}.{snvtype}{mutationclass}.vcf.fpfilter.var" % (config['dirs']['intermediate'], config['stepnames']['varscan_fpfilter']),
        bam=lambda wildcards: "%s%s%s/%s.reCal.reAl.nodup.srt.bam" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['gatk_PrintReads'], get_role(wildcards.entity.split('/')[0], wildcards.entity.split('/')[-1], 'tumor', SAMPLESHEETS)),
    output:
        "{prefix}%s%s/{entity}.{snvtype,snp|indel}{mutationclass,\.Somatic\.hc|\.Germline\.hc|}.vcf.fpfilter.var.readcounts" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount_somatic']),
    log:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.log" % (config['dirs']['logs'], config['stepnames']['bam_readcount_somatic']),
    benchmark:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['bam_readcount_somatic']),
    conda:
        "envs/spike_bam-readcount.yaml"
    threads:
        1
    shell:
        'bam-readcount'
        ' -q 1'
        ' -b 15'
        ' -w 1'
        ' -l {input.var}'
        ' -f {input.references[0]}'
        ' {input.bam}'
        ' > {output}'
        ' 2>> {log}'

        # /data/biotools/bin/bam-readcount -q1 -b15 -w1 -l $OUTPUTBASENAME.snp.Somatic.hc.vcf.fpfilter.var -f $REF $TUMOR_BAM >$OUTPUTBASENAME.snp.Somatic.hc.vcf.fpfilter.var.readcounts
        # /data/biotools/bin/bam-readcount -q1 -b15 -w1 -l $OUTPUTBASENAME.snp.Germline.hc.vcf.fpfilter.var -f $REF $TUMOR_BAM >$OUTPUTBASENAME.snp.Germline.hc.vcf.fpfilter.var.readcounts


rule somatic_FPfilter:
    input:
        script_fpfilter="{prefix}%sfpfilter.pl" % config['dirs']['references'],
        fixed="{prefix}%s%s/{entity}.{snvtype}{mutationclass}.corrected.vcf" % (config['dirs']['intermediate'], config['stepnames']['correct_genotypes']),
        readcounts="{prefix}%s%s/{entity}.{snvtype}{mutationclass}.vcf.fpfilter.var.readcounts" % (config['dirs']['intermediate'], config['stepnames']['bam_readcount_somatic']),
    output:
        "{prefix}%s%s/{entity}.{snvtype,snp|indel}{mutationclass,\.Somatic\.hc|\.Germline\.hc|}.corrected.fpfilter.tsv" % (config['dirs']['intermediate'], config['stepnames']['somatic_FPfilter']),
    log:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.log" % (config['dirs']['logs'], config['stepnames']['somatic_FPfilter']),
    benchmark:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['somatic_FPfilter']),
    threads:
        1
    shell:
        "perl"
        " {input.script_fpfilter}"
        " --var-file {input.fixed}"
        " --readcount-file {input.readcounts}"
        " --output-file {output}"
        " 2> {log} 1>&2"

        # ## [SNV] Execute FP filter
        # perl /data/Michael/software/variant-filter-master/fpfilter.pl --var-file $OUTPUTBASENAME.snp.Somatic.hc.fixed.vcf --readcount-file $OUTPUTBASENAME.snp.Somatic.hc.vcf.fpfilter.var.readcounts --output-file $OUTPUTBASENAME.snp.hc.corrected.fpfilter.tsv
        # ## [SNV] Execute FP filter on germline SNVs
        # perl /data/Michael/software/variant-filter-master/fpfilter.pl --var-file $OUTPUTBASENAME.snp.Germline.hc.fixed.vcf --readcount-file $OUTPUTBASENAME.snp.Germline.hc.vcf.fpfilter.var.readcounts --output-file $OUTPUTBASENAME.snp.Germline.hc.corrected.fpfilter.tsv


rule somatic_reduce:
    input:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.corrected.fpfilter.tsv" % (config['dirs']['intermediate'], config['stepnames']['somatic_FPfilter']),
    output:
        gz="{prefix}%s%s/{entity}.{snvtype,snp|indel}{mutationclass,\.Somatic\.hc|\.Germline\.hc|}.corrected.fpfilter.reduced.tsv.gz" % (config['dirs']['intermediate'], config['stepnames']['somatic_reduce']),
        idx="{prefix}%s%s/{entity}.{snvtype,snp|indel}{mutationclass,\.Somatic\.hc|\.Germline\.hc|}.corrected.fpfilter.reduced.tsv.gz.tbi" % (config['dirs']['intermediate'], config['stepnames']['somatic_reduce']),
    log:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.log" % (config['dirs']['logs'], config['stepnames']['somatic_reduce']),
    benchmark:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['somatic_reduce']),
    threads:
        1
    conda:
        "envs/spike_somatic_reduce.yaml"
    shell:
        "awk"
        " -v OFS='\t'"
        " '{{print $1, $2, $8, $9}}'"
        " {input}"
        " 2> {log}"
        " | bgzip -c > {output.gz} 2>> {log}"
        " && tabix"
        " -s 1"
        " -b 2"
        " -e 2"
        " {output.gz}"
        " 2>> {log}"

        # ## Use FP Filter output to annotate the FILTER column
        # ### [SNV] Reduce FP Filter output to neccessary columns
        # awk -v OFS='\t' '{print $1, $2, $8, $9}' $OUTPUTBASENAME.snp.hc.corrected.fpfilter.tsv > $OUTPUTBASENAME.snp.hc.corrected.fpfilter.reduced.tsv
        # awk -v OFS='\t' '{print $1, $2, $8, $9}' $OUTPUTBASENAME.snp.Germline.hc.corrected.fpfilter.tsv > $OUTPUTBASENAME.snp.Germline.hc.corrected.fpfilter.reduced.tsv
        # ### [SNV] Index the FP Filter file
        # bgzip $OUTPUTBASENAME.snp.corrected.fpfilter.reduced.tsv
        # bgzip $OUTPUTBASENAME.snp.Germline.corrected.fpfilter.reduced.tsv
        # tabix -s 1 -b 2 -e 2 $OUTPUTBASENAME.snp.hc.corrected.fpfilter.reduced.tsv.gz
        # tabix -s 1 -b 2 -e 2 $OUTPUTBASENAME.snp.Germline.hc.corrected.fpfilter.reduced.tsv.gz


rule vcf_annotate:
    input:
        script_filter="{prefix}%sfilter_freq_difference.rb" % config['dirs']['references'],
        vcf=lambda wildcards: "%s%s%s/%s.%s%s.vcf" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['varscan_somatic'] if wildcards.mutationclass=="" else config['stepnames']['varscan_processSomatic'], wildcards.entity, wildcards.snvtype, wildcards.mutationclass),
        tsv="{prefix}%s%s/{entity}.{snvtype}{mutationclass}.corrected.fpfilter.reduced.tsv.gz" % (config['dirs']['intermediate'], config['stepnames']['somatic_reduce']),
    output:
        "{prefix}%s%s/{entity}.{snvtype,snp|indel}{mutationclass,\.Somatic\.hc|\.Germline\.hc|}.corrected.varscan.somatic.vcf" % (config['dirs']['intermediate'], config['stepnames']['vcf_annotate']),
    log:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.log" % (config['dirs']['logs'], config['stepnames']['vcf_annotate']),
    benchmark:
        "{prefix}%s%s/{entity}.{snvtype}{mutationclass}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['vcf_annotate']),
    conda:
        "envs/spike_vcftools.yaml"
    threads:
        1
    shell:
        # something wired is going on with the GPFS file system, such that the
        # *.tbi index file seems to be older than the file for which it is an
        # index. Therefore, I touch the index here to update the timestamp.
        "touch {input.tsv}.tbi 2> {log}"
        ' && if [[ "{input.vcf}" = *"Somatic.hc"* ]]; then pattern="SS=2"; else pattern="SOMATIC"; fi'
        ' && if [[ "{input.vcf}" = *"Germline.hc"* ]]; then pattern="SS=1"; fi'
        " && cat {input.vcf}  2>> {log}"
        " | {input.script_filter}"
        " -freq 9 2>> {log}"
        " | vcf-annotate"
        " -a {input.tsv}"
        ' -d "key=INFO,ID=VSFP,Number=1,Type=String,Description=\'VarScan2 FP filter annotation\'"'
        " -c CHROM,POS,FILTER,INFO/VSFP"
        " 2>> {log}"
        ' | grep -e "^#" -e "$pattern" 2>> {log}'
        ' | grep -e "^#" -e "PASS" 2>> {log}'
        " > {output}"

        # ### [SNV] Actually use vcf-annotate to add the annotation to the filter field
        # ### [SNV] ALSO: Filter SOMATIC and PASS variants for final VCF output
        # cat $OUTPUTBASENAME.snp.Somatic.hc.vcf |
        # /data/biotools/scripts/filter_freq_difference.rb -freq 9 | ## make sure we only consider SNVs that have at least 10% frequency difference between tumor and normal
        # vcf-annotate -a $OUTPUTBASENAME.snp.hc.corrected.fpfilter.reduced.tsv.gz \
        # -d "key=INFO,ID=VSFP,Number=1,Type=String,Description='VarScan2 FP filter annotation'" \
        # -c CHROM,POS,FILTER,INFO/VSFP | grep -e "^#" -e "SS=2" | grep -e "^#" -e "PASS" > $OUTPUTBASENAME.snp.hc.corrected.varscan.somatic.vcf
        #
        # cat $OUTPUTBASENAME.snp.Germline.hc.vcf |
        # /data/biotools/scripts/filter_freq_difference.rb -freq 9 | ## make sure we only consider SNVs that have at least 10% frequency difference between tumor and normal
        # vcf-annotate -a $OUTPUTBASENAME.snp.Germline.hc.corrected.fpfilter.reduced.tsv.gz \
        # -d "key=INFO,ID=VSFP,Number=1,Type=String,Description='VarScan2 FP filter annotation'" \
        # -c CHROM,POS,FILTER,INFO/VSFP | grep -e "^#" -e "SS=1" | grep -e "^#" -e "PASS" > $OUTPUTBASENAME.snp.Germline.hc.corrected.varscan.somatic.vcf


rule merge_somatic_mus_musculus:
    # Use vcf-concat to combine SNP, INDEL and LOH calls
    input:
        snp="{prefix}%s%s/{entity}.snp.corrected.varscan.somatic.vcf" % (config['dirs']['intermediate'], config['stepnames']['vcf_annotate']),
        indel="{prefix}%s%s/{entity}.indel.corrected.varscan.somatic.vcf" % (config['dirs']['intermediate'], config['stepnames']['vcf_annotate']),
    output:
        "{prefix}%s%s/{entity}.indel_snp.vcf" % (config['dirs']['intermediate'], config['stepnames']['merge_somatic']),
    log:
        "{prefix}%s%s/{entity}.log" % (config['dirs']['logs'], config['stepnames']['merge_somatic']),
    benchmark:
        "{prefix}%s%s/{entity}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['merge_somatic']),
    conda:
        "envs/spike_vcftools.yaml"
    threads:
        1
    shell:
        "vcf-concat"
        " {input.snp}"
        " {input.indel}"
        ' > {output}'
        " 2> {log}"

        # vcf-concat
        # 	$OUTPUTBASENAME.snp.corrected.varscan.somatic.vcf
        # 	$OUTPUTBASENAME.indel.corrected.varscan.somatic.vcf > "$OUTPUTBASENAME.indel_snp.vcf"


rule merge_somatic_homo_sapiens:
    # Use vcf-concat to combine SNP, INDEL and LOH calls
    input:
        somatic="{prefix}%s%s/{entity}.snp.Somatic.hc.corrected.varscan.somatic.vcf" % (config['dirs']['intermediate'], config['stepnames']['vcf_annotate']),
        germline="{prefix}%s%s/{entity}.snp.Germline.hc.corrected.varscan.somatic.vcf" % (config['dirs']['intermediate'], config['stepnames']['vcf_annotate']),
    output:
        "{prefix}%s%s/{entity}.snp.somatic_germline.vcf" % (config['dirs']['intermediate'], config['stepnames']['merge_somatic']),
    log:
        "{prefix}%s%s/{entity}.log" % (config['dirs']['logs'], config['stepnames']['merge_somatic']),
    benchmark:
        "{prefix}%s%s/{entity}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['merge_somatic']),
    conda:
        "envs/spike_vcftools.yaml"
    threads:
        1
    shell:
        "vcf-concat"
        " {input.somatic}"
        " {input.germline}"
        ' > {output}'
        " 2> {log}"

        # vcf-concat
        # 	$OUTPUTBASENAME.snp.hc.corrected.varscan.somatic.vcf
        # 	$OUTPUTBASENAME.snp.Germline.hc.corrected.varscan.somatic.vcf > "$OUTPUTBASENAME.indel_snp.hc.vcf"
