rule check_complete:
    input:
        "{prefix}%s%s{run}/RTAComplete.txt" % (config['dirs']['inputs'], config['dirs']['rawillumina'])
    output:
        fp_check="{prefix}%s%s/{run}.complete.txt" % (config['dirs']['intermediate'], config['stepnames']['check_complete'])
    log:
        "{prefix}%s%s/{run}.log" % (config['dirs']['logs'], config['stepnames']['check_complete'])
    benchmark:
        "{prefix}%s%s/{run}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['check_complete'])
    run:
        with open(output.fp_check, 'w') as f:
            f.write("Raw Illumina data from run are complete.\n")


rule split_demultiplex:
    input:
        fp_samplesheet="{prefix}%s%s{run}_spike.csv" % (config['dirs']['inputs'], config['dirs']['samplesheets']),
        status="{prefix}%s%s/{run}.complete.txt" % (config['dirs']['intermediate'], config['stepnames']['check_complete']),
    output:
        directory("{prefix}%s%s/{run}" % (config['dirs']['intermediate'], config['stepnames']['split_demultiplex']))
    log:
        "{prefix}%s%s/{run}.log" % (config['dirs']['logs'], config['stepnames']['split_demultiplex'])
    benchmark:
        "{prefix}%s%s/{run}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['split_demultiplex'])
    threads:
        1
    run:
        split_samplesheets(parse_samplesheet(input.fp_samplesheet), config, dry=False)


rule demultiplex:
    input:
        dir_samplesheets=directory("{prefix}%s%s/{run}" % (config['dirs']['intermediate'], config['stepnames']['split_demultiplex'])),
        raw=directory("{prefix}%s%s{run}" % (config['dirs']['inputs'], config['dirs']['rawillumina'])),
    output:
        directory("{prefix}%s%s/{run}/{part}" % (config['dirs']['intermediate'], config['stepnames']['demultiplex'])),
    log:
        "{prefix}%s%s/{run}_{part}.log" % (config['dirs']['logs'], config['stepnames']['demultiplex'])
    benchmark:
        "{prefix}%s%s/{run}_{part}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['demultiplex'])
    threads:
        8
    conda:
        "envs/spike_demux.yaml"
    params:
        samplesheet="{prefix}%s%s/{run}/samplesheet_{part}.csv" % (config['dirs']['intermediate'], config['stepnames']['split_demultiplex']),
    shell:
        # A little bit of magic is necessary to deal with colliding barcodes.
        # During sample sheet splitting, separat sheets are generated for samples that are marked in column "spike_notes" with "colliding barcode".
        # If lines in the [Data] table are found holding the value "colliding barcode", bcl2fastq is executed with --barcode-mismatches 0
        'rowdata=`grep "^\[Data\]$" -n {params.samplesheet} | cut -d ":" -f 1`'
        ' && num_colliding=`tail -n+$rowdata {params.samplesheet} | grep "colliding barcode" -c; true`'
        " && bcl2fastq"
        " --runfolder-dir {input.raw}/"
        " --output-dir {output}/"
        " --ignore-missing-bcls"
        " --sample-sheet {params.samplesheet}"
        " --loading-threads {threads}"
        " --processing-threads {threads}"
        " --writing-threads {threads}"
        ' `if [ "$num_colliding" -ge 1 ]; then echo " --barcode-mismatches 0 "; fi`'
        " 2>> {log}"
        " 1>&2"


rule join_demultiplex:
    input:
        demux_dirs=lambda wildcards: [directory("%s%s%s/%s/part_%i" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['demultiplex'], wildcards.run, p+1)) for p in range(split_samplesheets(parse_samplesheet("%s%s%s%s_spike.csv" % (wildcards.prefix, config['dirs']['inputs'], config['dirs']['samplesheets'], wildcards.run)), config, dry=True))],
    output:
        directory("{prefix}%s%s/{run}" % (config['dirs']['intermediate'], config['stepnames']['join_demultiplex']))
    log:
        "{prefix}%s%s/{run}.log" % (config['dirs']['logs'], config['stepnames']['join_demultiplex'])
    benchmark:
        "{prefix}%s%s/{run}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['join_demultiplex'])
    threads:
        1
    shell:
        "mkdir -p {output} -v 2> {log} 1>&2"
        " && for inp in `echo \"{input}\"`; do "
        #"    cp -v $inp/Stats/Stats.json {output}/Stats_`basename $inp`.json 2>> {log} 1>&2;"
        "    for d in `find $inp -maxdepth 1 -mindepth 1 -type d ! -path '*Stats' ! -path '*Reports'`; do"
        "        cp -l -r -v $d {output} 2>> {log} 1>&2;"
        "    done;"
        " done"
        # rename demux fastq files such that S\d+ are removed from filenames
        ' && for f in `find {output}/ -name "*.fastq.gz"`; do '
        '    fnoS=`echo "$f" | sed -r "s/_S[0-9]+_/_/"`; mv -v $f $fnoS 2>> {log} 1>&2; '
        ' done'


rule yield_report:
    input:
        demux_dirs=lambda wildcards: [directory("%s%s%s/%s/part_%i" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['demultiplex'], wildcards.run, p+1)) for p in range(split_samplesheets(parse_samplesheet("%s%s%s%s_spike.csv" % (wildcards.prefix, config['dirs']['inputs'], config['dirs']['samplesheets'], wildcards.run)), config, dry=True))],
    output:
        report="{prefix}%s%s/Reports/{run}.yield_report.html" % (config['dirs']['intermediate'], config['stepnames']['yield_report']),
        yielddata="{prefix}%s%s/Data/{run}.yield_data.csv" % (config['dirs']['intermediate'], config['stepnames']['yield_report']),
    log:
        "{prefix}%s%s/{run}.log" % (config['dirs']['logs'], config['stepnames']['yield_report'])
    benchmark:
        "{prefix}%s%s/{run}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['yield_report'])
    threads:
        1
    run:
        dir_demux = "%s%s%s/%s" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['demultiplex'], wildcards.run)
        # collect necessary data
        lane_meta, lane_summary, top_unknown_barcodes = collect_yield_data(dir_demux)
        # create yield report as HTML file
        create_html_yield_report(output.report, lane_meta, lane_summary, top_unknown_barcodes, config)
        # create a file containing yields for status_update reporting
        yield_data = lane_summary[['Lane', 'Project', 'Sample', 'Yield', 'Barcode sequence']]
        yield_data[yield_data['Sample'] != 'Undetermined'].to_csv(output.yielddata, sep="\t", index=False)


rule aggregate_undetermined_filesizes:
    input:
        directory("{prefix}%s%s/{run}" % (config['dirs']['intermediate'], config['stepnames']['demultiplex']))
    output:
        sizes="{prefix}%s%s/{run}.undetermined_filesizes.txt" % (config['dirs']['intermediate'], config['stepnames']['aggregate_undetermined_filesizes']),
        fp_check="{prefix}%s%s/{run}.checked.txt" % (config['dirs']['intermediate'], config['stepnames']['aggregate_undetermined_filesizes'])
    log:
        "{prefix}%s%s/{run}.log" % (config['dirs']['logs'], config['stepnames']['aggregate_undetermined_filesizes'])
    benchmark:
        "{prefix}%s%s/{run}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['aggregate_undetermined_filesizes'])
    params:
        fps_undertermined="{prefix}%s%s/{run}/Undetermined_L*_001.fastq.gz" % (config['dirs']['intermediate'], config['stepnames']['demultiplex'])
    shell:
        "stat -c '%s\t%n\tunknown' {params.fps_undertermined} > {output.sizes} && "
        "echo 'done.' > {output.fp_check}"


rule check_undetermined_filesizes:
    input:
        "{prefix}%s%s/{run}.undetermined_filesizes.txt" % (config['dirs']['intermediate'], config['stepnames']['aggregate_undetermined_filesizes'])
    output:
        "{prefix}%s%s/{run}.undetermined-filesizes.pdf" % (config['dirs']['intermediate'], config['stepnames']['check_undetermined_filesizes'])
    log:
        "{prefix}%s%s/{run}.log" % (config['dirs']['logs'], config['stepnames']['check_undetermined_filesizes'])
    benchmark:
        "{prefix}%s%s/{run}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['check_undetermined_filesizes'])
    run:
        report_undertermined_filesizes(input[0], output[0], os.path.join(wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['check_undetermined_filesizes'], wildcards.run, 'error_undetermined-filesizes.pdf'))


def _add_firstbasereport(wildcards, config, field='fp_firstbasereport'):
    # NextSeq runs do not contain a first base report file.
    # Thus, based on the run ID, we need to decide if we want to add the first
    # base report to the email.
    result = dict()

    # by default, we assume to have a HiSeq run, which has the first base report.
    result['fp_firstbasereport'] = "%s%s%s%s/First_Base_Report.htm" % (wildcards.prefix, config['dirs']['inputs'], config['dirs']['rawillumina'], wildcards.run)
    result['param_attachment'] = '%s %s' % (config['mail_attachment_parameter'], result['fp_firstbasereport'])
    result['email_text'] = ' and the first base report'

    # if the run name contains a specific pattern, we infer to have a MiSeq run at hand,
    # which does not have a first base report.
    if '_NB552271_' in wildcards.run:
        # we need to hack snakemake here and provide a real file,
        # in this case the yield report file which is than added "twice" as input.
        result['fp_firstbasereport'] = [] #"%s%s%s/Reports/%s.yield_report.html" % (wildcards.prefix, config['dirs']['intermediate'], config['stepnames']['yield_report'], wildcards.run)
        result['param_attachment'] = ""
        result['email_text'] = ""

    return result[field]
rule convert_illumina_report:
    input:
        yield_report="{prefix}%s%s/Reports/{run}.yield_report.html" % (config['dirs']['intermediate'], config['stepnames']['yield_report']),
        firstbase=lambda wildcards: _add_firstbasereport(wildcards, config),
    output:
        "{prefix}%s%s/{run}.yield_report.pdf" % (config['dirs']['intermediate'], config['stepnames']['convert_illumina_report'])
    log:
        "{prefix}%s%s/{run}.log" % (config['dirs']['logs'], config['stepnames']['convert_illumina_report'])
    benchmark:
        "{prefix}%s%s/{run}.benchmark" % (config['dirs']['benchmarks'], config['stepnames']['convert_illumina_report'])
    conda:
        "envs/spike_report.yaml"
    params:
        recipient=config['emails']['demultiplexreport'].split(',')[0],
        suggested_recipients='\n'.join(config['emails']['demultiplexreport'].split(',')[1:]),
        text_attachments=lambda wildcards: _add_firstbasereport(wildcards, config, 'email_text'),
        attachment_firstbasereport=lambda wildcards: _add_firstbasereport(wildcards, config, 'param_attachment'),
    shell:
        "wkhtmltopdf --orientation Landscape {input.yield_report} {output} 2> {log} &&"
        "echo 'suggested recipients:\n{params.suggested_recipients}\n\nHi there,\n\n"
        "this is an automated message from {config[name_program]}.\n\n"
        "Demultiplexing for flowcell {wildcards.run} finished. Please find attached the yield report{params.text_attachments}.\n\n"
        "Have a nice day!' | mail -s '[{config[name_program]}] demultiplex report {wildcards.run}' {config[mail_attachment_parameter]} {output} {params.attachment_firstbasereport} {params.recipient}"
        " 2>> {log}"
